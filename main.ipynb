{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZzv25vDL9LQ"
      },
      "source": [
        "# **Crypto market capitalization forecast based on S&P 500.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP7fynuIMGlg"
      },
      "source": [
        "## **Abstract**\n",
        "   Abstract here. Give an executive summary of your project: goal, methods, results, conclusions. Usually no more than 200 words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBGDBctaMYYQ"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "Here you have to explain the problem that you are solving. Explain why it is important, and what are the main challenges. Mention previous attempts (add papers as references) to solve it. Mainly focus on the techniques closely related to our approach. Briefly describe your approach and explain why it is promising for solving the addressed problem. Mention the dataset and the main results achieved.\n",
        "\n",
        "In this section, you can add **text** and **figures**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtzOOJDnMpWp"
      },
      "source": [
        "## **Methodology**\n",
        "Describe the important steps you took to achieve your goal. Focus more on the most important steps (preprocessing, extra features, model aspects) that turned out to be important. Mention the original aspects of the project and state how they relate to existing work.\n",
        "\n",
        "In this section, you can add **text** and **figures**. For instance, it is strongly suggested to add a picture of the best machine learning model that you implemented to solve your problem (and describe it).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step in our methodology involved preprocessing the raw data from three CSV files: one containing cryptocurrency market data (e.g., Aave), another one containing S&P 500 historical data, and the final one containing the fear index of the S&P500 (VIX). For the cryptocurrency data, we focused on key features such as Date, Volume, and Marketcap. Similarly, for the S&P 500 data, we retained relevant columns like Date, Open, High, Low, Close, Volume, and additional info regarding the fear index. The datasets were cleaned to handle missing values, if any, unwanted data and the Date columns were standardized to ensure compatibility for merging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First Let's import the necessary libraries that we need for the project!  \n",
        "Run the code below...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the libraries imported, we can now load the data and take a look at the first few rows along with some additional info by running the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Crypto Dataset Preview:\n",
            "   Sno  Name Symbol                Date       High        Low       Open  \\\n",
            "0    1  Aave   AAVE 2020-10-05 23:59:00  55.112358  49.787900  52.675035   \n",
            "1    2  Aave   AAVE 2020-10-06 23:59:00  53.402270  40.734578  53.291969   \n",
            "2    3  Aave   AAVE 2020-10-07 23:59:00  42.408314  35.970690  42.399947   \n",
            "3    4  Aave   AAVE 2020-10-08 23:59:00  44.902511  36.696057  39.885262   \n",
            "4    5  Aave   AAVE 2020-10-09 23:59:00  47.569533  43.291776  43.764463   \n",
            "\n",
            "       Close        Volume     Marketcap  \n",
            "0  53.219243  0.000000e+00  8.912813e+07  \n",
            "1  42.401599  5.830915e+05  7.101144e+07  \n",
            "2  40.083976  6.828342e+05  6.713004e+07  \n",
            "3  43.764463  1.658817e+06  2.202651e+08  \n",
            "4  46.817744  8.155377e+05  2.356322e+08  \n",
            "\n",
            "Stock Dataset Preview:\n",
            "        Date       Open       High        Low      Close   Volume  Day  \\\n",
            "0 1993-01-29  24.543517  24.543517  24.421410  24.526073  1003200   29   \n",
            "1 1993-02-01  24.543515  24.700510  24.543515  24.700510   480500    1   \n",
            "2 1993-02-02  24.683072  24.770292  24.630741  24.752848   201300    2   \n",
            "3 1993-02-03  24.787724  25.031938  24.770280  25.014494   529400    3   \n",
            "4 1993-02-04  25.101718  25.171493  24.822616  25.119162   531500    4   \n",
            "\n",
            "   Weekday  Week  Month  Year  \n",
            "0        4     4      1  1993  \n",
            "1        0     5      2  1993  \n",
            "2        1     5      2  1993  \n",
            "3        2     5      2  1993  \n",
            "4        3     5      2  1993  \n",
            "\n",
            "VIX Dataset Preview:\n",
            "        Date       Open       High        Low      Close\n",
            "0 1990-01-02  17.240000  17.240000  17.240000  17.240000\n",
            "1 1990-01-03  18.190001  18.190001  18.190001  18.190001\n",
            "2 1990-01-04  19.219999  19.219999  19.219999  19.219999\n",
            "3 1990-01-05  20.110001  20.110001  20.110001  20.110001\n",
            "4 1990-01-08  20.260000  20.260000  20.260000  20.260000\n",
            "\n",
            "Crypto Dataset ==> Min Date: 2013-04-29 23:59:00 / Max Date: 2021-07-06 23:59:00\n",
            "\n",
            "Stock Dataset ==> Min Date: 1993-01-29 00:00:00 / Max Date: 2025-02-28 00:00:00\n",
            "\n",
            "VIX Dataset ==> Min Date: 1990-01-02 00:00:00 / Max Date: 2024-06-24 00:00:00\n"
          ]
        }
      ],
      "source": [
        "KAGGLE_DATA_PATH = 'KaggleData/'\n",
        "\n",
        "def load_data(filename: str, date_col: str, date_format: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads a CSV file into a pandas DataFrame and parses the date column.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Name of the CSV file.\n",
        "        date_col (str): Name of the date column.\n",
        "        date_format (str): Format of the date in the CSV.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed DataFrame with parsed dates.\n",
        "    \"\"\"\n",
        "    filepath = os.path.join(KAGGLE_DATA_PATH, filename)\n",
        "    df = pd.read_csv(filepath)\n",
        "    df[date_col] = pd.to_datetime(df[date_col], format=date_format)\n",
        "    return df\n",
        "\n",
        "crypto_df = load_data('All_Crypto.csv', 'Date', '%d-%m-%Y %H:%M')\n",
        "stock_df = load_data('spy.csv', 'Date', '%Y-%m-%d')\n",
        "vix_df = load_data('vix_daily.csv', 'Date', '%Y-%m-%d')\n",
        "\n",
        "for name, df in zip([\"Crypto\", \"Stock\", \"VIX\"], [crypto_df, stock_df, vix_df]):\n",
        "    print(f\"\\n{name} Dataset Preview:\")\n",
        "    print(df.head())\n",
        "\n",
        "for name, df in zip([\"Crypto\", \"Stock\", \"VIX\"], [crypto_df, stock_df, vix_df]):\n",
        "    print(f\"\\n{name} Dataset ==> Min Date: {df['Date'].min()} / Max Date: {df['Date'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Crypto data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us focus on preprocessing the crypto data first. We can tackle afterwards the S&P500.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us see how many crypto coins we are dealing with by running the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Unique Cryptocurrencies: 23\n",
            "Aave, Binance Coin, Bitcoin, Cardano, Chainlink, Cosmos, Crypto.com Coin, Dogecoin, EOS, Ethereum, IOTA, Litecoin, Monero, NEM, Polkadot, Solana, Stellar, Tether, TRON, Uniswap, USD Coin, XRP, Wrapped Bitcoin\n"
          ]
        }
      ],
      "source": [
        "unique_crypto_names = crypto_df['Name'].unique()\n",
        "print(f\"Total Unique Cryptocurrencies: {len(unique_crypto_names)}\")\n",
        "print(\", \".join(unique_crypto_names)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 23 crypto currencies in the data we are using. Even though we would have preferred to have data about all the top 100 crypto currencies, this should also be fine for the scope of this project.  \n",
        "However, there are some cryptocurrencies that we would want to discard here like stable coins and currencies that are not in the top 100.  \n",
        "The reason we want to discard stablecoins is because they are designed to maintain a fixed value (usually pegged to the USD), so their market cap changes are primarily driven by issuance and redemption rather than market speculation or macroeconomic factors. Including them might introduce noise rather than meaningful predictive signals.  \n",
        "In this case, we are discarding `Crypto.com Coin`, `NEM`, `Tether`, `USD Coin`, and `Wrapped Bitcoin`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remaining Cryptocurrencies After Filtering: 18\n"
          ]
        }
      ],
      "source": [
        "EXCLUDE_LIST = {'Crypto.com Coin', 'NEM', 'Tether', 'USD Coin', 'Wrapped Bitcoin'}\n",
        "crypto_df_filtered = crypto_df[~crypto_df['Name'].isin(EXCLUDE_LIST)]\n",
        "print(f\"Remaining Cryptocurrencies After Filtering: {crypto_df_filtered['Name'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We should now take a look at the completness of the data from a range that I have educationally chosen (2018-01-01 to 2021-06-07).  \n",
        "By running the two cells below, we notice that some crypto currencies have been created after 2018, thus they would have no data from the year 2018 to the year that they have been created.  \n",
        "This is an issue.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cryptos with complete data: 13 -> ['Binance Coin', 'Bitcoin', 'Cardano', 'Chainlink', 'Dogecoin', 'EOS', 'Ethereum', 'IOTA', 'Litecoin', 'Monero', 'Stellar', 'TRON', 'XRP']\n",
            "Cryptos missing some dates: 5 -> ['Aave', 'Cosmos', 'Polkadot', 'Solana', 'Uniswap']\n"
          ]
        }
      ],
      "source": [
        "date_range = pd.date_range(start='2018-01-01', end='2021-06-07', freq='D')\n",
        "\n",
        "recent_cryptos = []\n",
        "original_cryptos = []\n",
        "\n",
        "# Group by 'Name' and check for missing dates\n",
        "for name, group in crypto_df_filtered.groupby('Name'):\n",
        "    unique_dates = group['Date'].dt.normalize().unique() \n",
        "    missing_dates = set(date_range) - set(unique_dates) \n",
        "\n",
        "    (recent_cryptos if missing_dates else original_cryptos).append(name)\n",
        "\n",
        "print(f\"Cryptos with complete data: {len(original_cryptos)} -> {original_cryptos}\")\n",
        "print(f\"Cryptos missing some dates: {len(recent_cryptos)} -> {recent_cryptos}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following this I have decided on doing the project in 2 different experiments:  \n",
        "1. Include all crypto currencies that have data that range from 2018 or earlier to 2021.  \n",
        "2. Include all crypto currencies that have data that range from the `last date a crypto was created` to 2021.  \n",
        "\n",
        "This means that `option 1` will have less crypto currency diversity, but have a longer time frame, and `option 2`, will have a wider  crypto currency diversity on a smaller time frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us create the dataset for `option 1`. (Full time frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset includes cryptos from 2017-10-02 to 2021-07-06 with 13 cryptos.\n"
          ]
        }
      ],
      "source": [
        "min_start_dates = crypto_df_filtered.loc[crypto_df_filtered['Name'].isin(original_cryptos)] \\\n",
        "                                   .groupby('Name')['Date'].min()\n",
        "latest_common_start = min_start_dates.max()\n",
        "\n",
        "max_date = crypto_df['Date'].max()\n",
        "\n",
        "full_time_frame_crypto_df = crypto_df_filtered[\n",
        "    (crypto_df_filtered['Name'].isin(original_cryptos)) &\n",
        "    (crypto_df_filtered['Date'].between(latest_common_start, max_date))\n",
        "]\n",
        "\n",
        "print(f\"Dataset includes cryptos from {latest_common_start.date()} to {max_date.date()} \"\n",
        "      f\"with {full_time_frame_crypto_df['Name'].nunique()} cryptos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now create the dataset for `option 2`. (Partial time frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset includes cryptos from 2020-10-05 to 2021-07-06 with 18 cryptos.\n"
          ]
        }
      ],
      "source": [
        "latest_start_date = crypto_df_filtered.groupby('Name')['Date'].min().max()\n",
        "partial_time_frame_crypto_df = crypto_df_filtered[crypto_df_filtered['Date'] >= latest_start_date]\n",
        "\n",
        "print(f\"Dataset includes cryptos from {latest_start_date.date()} to {partial_time_frame_crypto_df['Date'].max().date()} \"\n",
        "      f\"with {partial_time_frame_crypto_df['Name'].nunique()} cryptos.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### S&P500 Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now process the stock market data.  \n",
        "From what we have accomplished before, we know that we need to match the date ranges of our crypto data. Which means we will have `full_time_frame_stock_df` and a `partial_time_frame_stock_df` that range from `2017-10-02` to `2021-07-06` and from `2020-10-05` to `2021-07-06` respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to keep in mind that the stock market closes on the weekends. Thus, for the sake of this project, we will assume that the last available price (Friday’s) carries over to Saturday and Sunday since stock prices don’t change on weekends.  \n",
        "We will use Forward fill to accomplish this.  \n",
        "This will keep the dataset aligned with the crypto data. Also, it reflects the reality that stock prices remain unchanged on weekends.\n",
        "Same thing will be done to the VIX data frame as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_start_date, partial_start_date, end_date = \"2017-10-02\", \"2020-10-05\", \"2021-07-06\"\n",
        "\n",
        "def handle_closed_weekends(df: pd.DataFrame, start_date: str, end_date: str, date_col: str = 'Date') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters a dataframe within a date range, expands it to include weekends,\n",
        "    and forward-fills missing values.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): The original dataframe with a date column.\n",
        "        start_date (str): The start date for filtering.\n",
        "        end_date (str): The end date for filtering.\n",
        "        date_col (str): The name of the date column. Defaults to 'Date'.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Processed data with weekends included and missing values filled.\n",
        "    \"\"\"\n",
        "    filtered_df = df[(df[date_col] >= start_date) & (df[date_col] <= end_date)].copy()\n",
        "\n",
        "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "    # Reindex to ensure weekends are included, then forward-fill missing values\n",
        "    return (\n",
        "        filtered_df.set_index(date_col)\n",
        "        .reindex(date_range)\n",
        "        .ffill()\n",
        "        .reset_index()\n",
        "        .rename(columns={'index': date_col})\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full S&P500 dataset: 2017-10-02 00:00:00 to 2021-07-06 00:00:00 with 1374 rows\n",
            "Partial S&P500 dataset: 2020-10-05 00:00:00 to 2021-07-06 00:00:00 with 275 rows\n"
          ]
        }
      ],
      "source": [
        "# Preprocess stock data\n",
        "full_time_frame_stock_df = handle_closed_weekends(stock_df, full_start_date, end_date)\n",
        "partial_time_frame_stock_df = handle_closed_weekends(stock_df, partial_start_date, end_date)\n",
        "\n",
        "# Print results\n",
        "print(f\"Full S&P500 dataset: {full_time_frame_stock_df['Date'].min()} to {full_time_frame_stock_df['Date'].max()} \"\n",
        "      f\"with {len(full_time_frame_stock_df)} rows\")\n",
        "print(f\"Partial S&P500 dataset: {partial_time_frame_stock_df['Date'].min()} to {partial_time_frame_stock_df['Date'].max()} \"\n",
        "      f\"with {len(partial_time_frame_stock_df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full VIX dataset: 2017-10-02 00:00:00 to 2021-07-06 00:00:00 with 1374 rows\n",
            "Partial VIX dataset: 2020-10-05 00:00:00 to 2021-07-06 00:00:00 with 275 rows\n"
          ]
        }
      ],
      "source": [
        "# Preprocess VIX data\n",
        "full_time_frame_vix_df = handle_closed_weekends(vix_df, full_start_date, end_date)\n",
        "partial_time_frame_vix_df = handle_closed_weekends(vix_df, partial_start_date, end_date)\n",
        "\n",
        "# Print results\n",
        "print(f\"Full VIX dataset: {full_time_frame_vix_df['Date'].min()} to {full_time_frame_vix_df['Date'].max()} \"\n",
        "      f\"with {len(full_time_frame_vix_df)} rows\")\n",
        "print(f\"Partial VIX dataset: {partial_time_frame_vix_df['Date'].min()} to {partial_time_frame_vix_df['Date'].max()} \"\n",
        "      f\"with {len(partial_time_frame_vix_df)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now save all the data frames as csvs. Might be useful later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROCESSED_DATA_PATH = 'ProcessedData/'\n",
        "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
        "\n",
        "def save_to_csv(df: pd.DataFrame, filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Saves a DataFrame to a CSV file in the processed data directory.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to save.\n",
        "        filename (str): The name of the CSV file.\n",
        "    \"\"\"\n",
        "    filepath = os.path.join(PROCESSED_DATA_PATH, filename)\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(f\"Saved {filename} to {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved full_time_frame_crypto_data.csv to ProcessedData/full_time_frame_crypto_data.csv\n",
            "Saved partial_time_frame_crypto_data.csv to ProcessedData/partial_time_frame_crypto_data.csv\n",
            "Saved full_time_frame_stock_data.csv to ProcessedData/full_time_frame_stock_data.csv\n",
            "Saved partial_time_frame_stock_data.csv to ProcessedData/partial_time_frame_stock_data.csv\n",
            "Saved full_time_frame_vix_data.csv to ProcessedData/full_time_frame_vix_data.csv\n",
            "Saved partial_time_frame_vix_data.csv to ProcessedData/partial_time_frame_vix_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Save all processed DataFrames\n",
        "save_to_csv(full_time_frame_crypto_df, 'full_time_frame_crypto_data.csv')\n",
        "save_to_csv(partial_time_frame_crypto_df, 'partial_time_frame_crypto_data.csv')\n",
        "save_to_csv(full_time_frame_stock_df, 'full_time_frame_stock_data.csv')\n",
        "save_to_csv(partial_time_frame_stock_df, 'partial_time_frame_stock_data.csv')\n",
        "save_to_csv(full_time_frame_vix_df, 'full_time_frame_vix_data.csv')\n",
        "save_to_csv(partial_time_frame_vix_df, 'partial_time_frame_vix_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Merging the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perfect, now that we have cleaned the data, we still have a couple more steps before starting to play with models.  \n",
        "Steps:  \n",
        "- Extract the global volume and global market cap for each day in the crypto data.\n",
        "- Merge the stock, vix, and crypto market data together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_crypto_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates crypto data by date, summing 'Volume' and 'Marketcap',\n",
        "    and renames the columns for clarity.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with 'Date', 'Volume', and 'Marketcap' columns.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Aggregated DataFrame with 'Date', 'Total_Volume', and 'Total_Market_Cap'.\n",
        "    \"\"\"\n",
        "    aggregated_df = (\n",
        "        df.groupby('Date', as_index=False)\n",
        "        .agg({'Volume': 'sum', 'Marketcap': 'sum'})\n",
        "        .rename(columns={'Volume': 'Total_Volume', 'Marketcap': 'Total_Market_Cap'})\n",
        "    )\n",
        "    # Remove time from the 'Date' column for better compactness\n",
        "    aggregated_df['Date'] = aggregated_df['Date'].dt.date\n",
        "    return aggregated_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full time frame crypto aggregate data: \n",
            "          Date  Total_Volume  Total_Market_Cap\n",
            "0  2017-10-02  2.084329e+09      1.166802e+11\n",
            "1  2017-10-03  1.857098e+09      1.145126e+11\n",
            "2  2017-10-04  1.648587e+09      1.134182e+11\n",
            "3  2017-10-05  1.965073e+09      1.162994e+11\n",
            "4  2017-10-06  1.711230e+09      1.180752e+11\n",
            "\n",
            "Partial time frame crypto aggregate data: \n",
            "          Date  Total_Volume  Total_Market_Cap\n",
            "0  2020-10-05  7.121521e+10      2.789262e+11\n",
            "1  2020-10-06  7.336479e+10      2.719265e+11\n",
            "2  2020-10-07  6.273427e+10      2.734531e+11\n",
            "3  2020-10-08  9.018118e+10      2.803745e+11\n",
            "4  2020-10-09  4.923914e+10      2.861705e+11 \n",
            "\n",
            "Saved full_time_frame_aggregate_crypto_data.csv to ProcessedData/full_time_frame_aggregate_crypto_data.csv\n",
            "Saved partial_time_frame_aggregate_crypto_data.csv to ProcessedData/partial_time_frame_aggregate_crypto_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Aggregate full and partial time frame crypto data\n",
        "full_time_frame_crypto_aggregated_df = aggregate_crypto_data(full_time_frame_crypto_df)\n",
        "partial_time_frame_crypto_aggregated_df = aggregate_crypto_data(partial_time_frame_crypto_df)\n",
        "\n",
        "print(\"Full time frame crypto aggregate data: \\n\", full_time_frame_crypto_aggregated_df.head())\n",
        "print(\"\\nPartial time frame crypto aggregate data: \\n\", partial_time_frame_crypto_aggregated_df.head(), '\\n')\n",
        "\n",
        "save_to_csv(full_time_frame_crypto_aggregated_df, 'full_time_frame_aggregate_crypto_data.csv')\n",
        "save_to_csv(partial_time_frame_crypto_aggregated_df, 'partial_time_frame_aggregate_crypto_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataset(df: pd.DataFrame, dataset_type: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes a dataset by converting the 'Date' column to datetime,\n",
        "    dropping unnecessary columns, and renaming columns for clarity.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        dataset_type (str): The type of dataset ('crypto', 'stock', or 'vix').\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Processed DataFrame.\n",
        "    \"\"\"\n",
        "    # Convert 'Date' column to datetime\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    \n",
        "    # Process based on dataset type\n",
        "    if dataset_type == 'stock':\n",
        "        df = df.drop(['Day', 'Weekday', 'Week', 'Month', 'Year'], axis=1)\n",
        "        df = df.rename(columns={\n",
        "            'Open': 'S&P500_Open',\n",
        "            'High': 'S&P500_High',\n",
        "            'Low': 'S&P500_Low',\n",
        "            'Close': 'S&P500_Close',\n",
        "            'Volume': 'S&P500_Volume'\n",
        "        })\n",
        "    elif dataset_type == 'crypto':\n",
        "        df = df.rename(columns={\n",
        "            'Total_Volume': 'Crypto_Volume',\n",
        "            'Total_Market_Cap': 'Crypto_Market_Cap'\n",
        "        })\n",
        "    elif dataset_type == 'vix':\n",
        "        df = df.rename(columns={\n",
        "            'Open': 'VIX_Open',\n",
        "            'High': 'VIX_High',\n",
        "            'Low': 'VIX_Low',\n",
        "            'Close': 'VIX_Close'\n",
        "        })\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid dataset type: {dataset_type}. Expected 'crypto', 'stock', or 'vix'.\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_time_frame_crypto_aggregated_df = process_dataset(full_time_frame_crypto_aggregated_df, 'crypto')\n",
        "full_time_frame_stock_df = process_dataset(full_time_frame_stock_df, 'stock')\n",
        "full_time_frame_vix_df = process_dataset(full_time_frame_vix_df, 'vix')\n",
        "\n",
        "partial_time_frame_crypto_aggregated_df = process_dataset(partial_time_frame_crypto_aggregated_df, 'crypto')\n",
        "partial_time_frame_stock_df = process_dataset(partial_time_frame_stock_df, 'stock')\n",
        "partial_time_frame_vix_df = process_dataset(partial_time_frame_vix_df, 'vix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now merge the crypto, stock, and VIX market data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged data for full time: \n",
            "         Date  Crypto_Volume  Crypto_Market_Cap  S&P500_Open  S&P500_High  \\\n",
            "0 2017-10-02   2.084329e+09       1.166802e+11   223.421919   224.159286   \n",
            "1 2017-10-03   1.857098e+09       1.145126e+11   224.159281   224.665658   \n",
            "2 2017-10-04   1.648587e+09       1.134182e+11   224.487941   225.154236   \n",
            "3 2017-10-05   1.965073e+09       1.162994e+11   225.243074   226.255841   \n",
            "4 2017-10-06   1.711230e+09       1.180752e+11   225.784976   226.273594   \n",
            "\n",
            "   S&P500_Low  S&P500_Close  S&P500_Volume  VIX_Open  VIX_High  VIX_Low  \\\n",
            "0  223.244229    224.159286     59023000.0      9.59     10.04     9.37   \n",
            "1  224.079316    224.639008     66810200.0      9.30      9.75     9.30   \n",
            "2  224.372446    224.905487     55953600.0      9.53      9.88     9.53   \n",
            "3  224.941024    226.238083     63522800.0      9.48      9.62     9.13   \n",
            "4  225.518469    225.980423     80646000.0      9.23     10.27     9.11   \n",
            "\n",
            "   VIX_Close  \n",
            "0       9.45  \n",
            "1       9.51  \n",
            "2       9.63  \n",
            "3       9.19  \n",
            "4       9.65  \n",
            "Merged data for partial time: \n",
            "         Date  Crypto_Volume  Crypto_Market_Cap  S&P500_Open  S&P500_High  \\\n",
            "0 2020-10-05   7.121521e+10       2.789262e+11   316.040724   319.708393   \n",
            "1 2020-10-06   7.336479e+10       2.719265e+11   319.661469   321.786850   \n",
            "2 2020-10-07   6.273427e+10       2.734531e+11   317.978037   321.278954   \n",
            "3 2020-10-08   9.018118e+10       2.803745e+11   322.426250   323.366679   \n",
            "4 2020-10-09   4.923914e+10       2.861705e+11   324.974796   326.658172   \n",
            "\n",
            "   S&P500_Low  S&P500_Close  S&P500_Volume   VIX_Open   VIX_High    VIX_Low  \\\n",
            "0  315.993714    319.520325     45713100.0  29.520000  29.690001  27.270000   \n",
            "1  314.460894    314.978119     90128900.0  28.049999  30.000000  26.010000   \n",
            "2  317.949825    320.460785     56999600.0  29.260000  29.760000  27.940001   \n",
            "3  321.495205    323.300842     45242500.0  27.650000  27.990000  24.879999   \n",
            "4  324.344724    326.187958     59528600.0  26.200001  26.219999  24.030001   \n",
            "\n",
            "   VIX_Close  \n",
            "0  27.959999  \n",
            "1  29.480000  \n",
            "2  28.059999  \n",
            "3  26.360001  \n",
            "4  25.000000  \n"
          ]
        }
      ],
      "source": [
        "merged_df1 = pd.merge(full_time_frame_crypto_aggregated_df, full_time_frame_stock_df, on='Date', how='inner')\n",
        "full_time_merged_df = pd.merge(merged_df1, full_time_frame_vix_df, on='Date', how='inner')\n",
        "\n",
        "merged_df2 = pd.merge(partial_time_frame_crypto_aggregated_df, partial_time_frame_stock_df, on='Date', how='inner')\n",
        "partial_time_merged_df = pd.merge(merged_df2, partial_time_frame_vix_df, on='Date', how='inner')\n",
        "\n",
        "print('Merged data for full time: \\n', full_time_merged_df.head())\n",
        "print('Merged data for partial time: \\n', partial_time_merged_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved full_time_merged_data.csv to ProcessedData/full_time_merged_data.csv\n",
            "Saved partial_time_merged_data.csv to ProcessedData/partial_time_merged_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Save the merged data\n",
        "save_to_csv(full_time_merged_df, 'full_time_merged_data.csv')\n",
        "save_to_csv(partial_time_merged_df, 'partial_time_merged_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**And we're done for the preprocessing part!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIQOcLeaPq3v"
      },
      "source": [
        "## **Experimental Setup**\n",
        "Describe the datasets used for your experiments. List the machine learning techniques used to solve your problem and report the corresponding hyperparameters.\n",
        "\n",
        "In this section, you can add **text**, **tables**, and **figures**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRL5KR20QWKu"
      },
      "source": [
        "## **Experimental Results**\n",
        "Describe here the main experimental results. Critically discuss them. Compare them with results available in the literature (if applicable).\n",
        "\n",
        "In this section, you can add **text** and **figures**, **tables**, **plots**, and code. Make sure the code is runnable and replicable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Jyn3BcQDpf"
      },
      "source": [
        "## **Conclusions**\n",
        "\n",
        "Summarize what you could and could not conclude based on your experiments.\n",
        "In this section, you can add **text**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaxqlm6kRcmb"
      },
      "source": [
        "## **References**\n",
        "You can add here the citations of books, websites, or academic papers, etc."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
